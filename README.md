# testproject

TD: This is a test generated by AI

import polars as pl
import numpy as np
from collections import Counter
from typing import Dict, List, Tuple
from scipy.stats import entropy as scipy_entropy
import re
from datetime import datetime, timedelta
import math

class FilingEntropyAnalyzer:
    """
    基于信息论的8-K Filing信号提取器
    核心思想：通过多维度的熵计算来量化filing的信息含量
    """
    
    def __init__(self, df: pl.DataFrame):
        """
        初始化分析器
        
        参数:
        df: 包含columns [qid, document_id, sentence_id, sentence_text, filing_date]
        """
        self.df = df
        self.stopwords = self._get_financial_stopwords()
        
    def _get_financial_stopwords(self) -> set:
        """获取金融领域的停用词"""
        basic_stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'as', 'is', 'was', 'are', 'were', 'be', 'been',
            'have', 'has', 'had', 'will', 'would', 'could', 'should', 'may', 'might',
            'this', 'that', 'these', 'those', 'such', 'said', 'says'
        }
        
        # 添加金融相关的常见词汇（这些词信息量较低）
        financial_common = {
            'company', 'corporation', 'inc', 'llc', 'ltd', 'securities', 'sec',
            'pursuant', 'accordance', 'herewith', 'thereof', 'hereby', 'whereas'
        }
        
        return basic_stopwords.union(financial_common)
    
    def compute_word_entropy(self, text: str) -> float:
        """
        计算单个文本的词汇熵
        H(X) = -Σ p(x) * log2(p(x))
        """
        # 文本预处理
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        words = [w for w in words if w not in self.stopwords and len(w) > 2]
        
        if not words:
            return 0.0
            
        # 计算词频
        word_counts = Counter(words)
        total_words = len(words)
        
        # 计算概率分布
        probabilities = [count / total_words for count in word_counts.values()]
        
        # 计算熵
        return scipy_entropy(probabilities, base=2)
    
    def compute_sentence_length_entropy(self, sentences: List[str]) -> float:
        """
        计算句子长度分布的熵
        长度分布的不均匀性可能反映信息的结构化程度
        """
        if not sentences:
            return 0.0
            
        lengths = [len(sentence.split()) for sentence in sentences]
        
        # 将长度分桶
        max_len = max(lengths)
        bins = np.linspace(0, max_len, min(10, max_len + 1))
        hist, _ = np.histogram(lengths, bins=bins)
        
        # 过滤掉空桶
        hist = hist[hist > 0]
        probabilities = hist / hist.sum()
        
        return scipy_entropy(probabilities, base=2)
    
    def compute_semantic_entropy(self, text: str) -> float:
        """
        计算语义熵 - 基于关键词类别的分布
        """
        # 定义不同类别的关键词
        categories = {
            'financial': ['revenue', 'profit', 'loss', 'earnings', 'cash', 'debt', 'asset', 'liability', 'dividend'],
            'legal': ['agreement', 'contract', 'lawsuit', 'litigation', 'settlement', 'compliance', 'regulation'],
            'operational': ['acquisition', 'merger', 'restructuring', 'expansion', 'closure', 'partnership'],
            'personnel': ['appointment', 'resignation', 'executive', 'director', 'officer', 'employment'],
            'market': ['stock', 'share', 'market', 'trading', 'price', 'valuation', 'investor'],
            'risk': ['risk', 'uncertainty', 'contingency', 'material', 'adverse', 'impact', 'exposure']
        }
        
        text_lower = text.lower()
        category_counts = {}
        
        for category, keywords in categories.items():
            count = sum(text_lower.count(keyword) for keyword in keywords)
            if count > 0:
                category_counts[category] = count
        
        if not category_counts:
            return 0.0
            
        total_matches = sum(category_counts.values())
        probabilities = [count / total_matches for count in category_counts.values()]
        
        return scipy_entropy(probabilities, base=2)
    
    def compute_temporal_surprise(self, target_date: str = None, lookback_days: int = 30) -> pl.DataFrame:
        """
        计算时间维度的惊喜度（相对熵）
        通过比较当前filing与历史filing的词汇分布差异
        支持一个document_id对应多个sentence_id的情况
        """
        if target_date is None:
            # 如果没有指定日期，使用最新的日期
            target_date = self.df['filing_date'].max()
        
        current_date_dt = datetime.strptime(target_date, '%Y-%m-%d')
        lookback_date = current_date_dt - timedelta(days=lookback_days)
        
        # 获取历史数据 - 按document聚合
        historical_docs = self.df.filter(
            (pl.col('filing_date') >= lookback_date.strftime('%Y-%m-%d')) &
            (pl.col('filing_date') < target_date)
        ).group_by(['document_id']).agg([
            pl.col('sentence_text').alias('sentences')
        ])
        
        current_docs = self.df.filter(
            pl.col('filing_date') == target_date
        ).group_by(['qid', 'document_id', 'filing_date']).agg([
            pl.col('sentence_text').alias('sentences')
        ])
        
        if historical_docs.is_empty() or current_docs.is_empty():
            return current_docs.with_columns([
                pl.lit(0.0).alias('temporal_surprise')
            ])
        
        # 构建历史词汇分布 - 聚合所有历史文档
        all_historical_sentences = []
        for row in historical_docs.iter_rows(named=True):
            sentences = row['sentences']
            if isinstance(sentences, list):
                all_historical_sentences.extend(sentences)
            else:
                all_historical_sentences.append(sentences)
        
        historical_text = ' '.join(all_historical_sentences)
        historical_words = re.findall(r'\b[a-zA-Z]+\b', historical_text.lower())
        historical_words = [w for w in historical_words if w not in self.stopwords and len(w) > 2]
        historical_dist = Counter(historical_words)
        
        # 计算当前每个document的KL散度
        surprise_scores = []
        
        for row in current_docs.iter_rows(named=True):
            doc_id = row['document_id']
            sentences = row['sentences']
            
            # 处理句子列表
            if isinstance(sentences, list):
                doc_text = ' '.join(sentences)
            else:
                doc_text = sentences if sentences else ""
            
            current_words = re.findall(r'\b[a-zA-Z]+\b', doc_text.lower())
            current_words = [w for w in current_words if w not in self.stopwords and len(w) > 2]
            current_dist = Counter(current_words)
            
            # 计算KL散度
            kl_div = self._compute_kl_divergence(current_dist, historical_dist)
            surprise_scores.append({
                'qid': row['qid'],
                'document_id': doc_id, 
                'filing_date': row['filing_date'],
                'temporal_surprise': kl_div
            })
        
        return pl.DataFrame(surprise_scores)
    
    def _compute_kl_divergence(self, p_dist: Counter, q_dist: Counter) -> float:
        """
        计算两个分布之间的KL散度
        KL(P||Q) = Σ P(x) * log(P(x)/Q(x))
        """
        if not p_dist or not q_dist:
            return 0.0
            
        # 获取所有词汇
        all_words = set(p_dist.keys()) | set(q_dist.keys())
        
        # 平滑处理
        alpha = 0.001
        p_total = sum(p_dist.values())
        q_total = sum(q_dist.values())
        
        kl_div = 0.0
        for word in all_words:
            p_prob = (p_dist.get(word, 0) + alpha) / (p_total + alpha * len(all_words))
            q_prob = (q_dist.get(word, 0) + alpha) / (q_total + alpha * len(all_words))
            
            kl_div += p_prob * math.log(p_prob / q_prob)
        
        return kl_div
    
    def compute_information_density(self, text: str) -> float:
        """
        计算信息密度 - 每个词的平均信息量
        """
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        words = [w for w in words if w not in self.stopwords and len(w) > 2]
        
        if not words:
            return 0.0
            
        # 使用逆文档频率作为信息量的代理
        word_counts = Counter(words)
        total_words = len(words)
        
        # 计算每个词的信息量 (负对数概率)
        information_content = 0.0
        for word, count in word_counts.items():
            prob = count / total_words
            information_content += count * (-math.log2(prob))
        
        return information_content / total_words
    
    def extract_entropy_signals(self) -> pl.DataFrame:
        """
        提取所有基于熵的信号
        支持一个document_id对应多个sentence_id的情况
        """
        print("开始提取熵信号...")
        
        # 验证数据结构
        total_sentences = len(self.df)
        unique_docs = self.df['document_id'].n_unique()
        print(f"总句子数: {total_sentences}, 唯一文档数: {unique_docs}")
        
        # 按document聚合句子，确保sentence_id排序
        doc_level_df = self.df.sort(['document_id', 'sentence_id']).group_by(['qid', 'document_id', 'filing_date']).agg([
            pl.col('sentence_text').alias('sentences'),
            pl.col('sentence_id').count().alias('sentence_count_check')
        ])
        
        print(f"聚合后文档数: {len(doc_level_df)}")
        
        # 计算各种熵指标
        entropy_signals = []
        
        for row in doc_level_df.iter_rows(named=True):
            sentences = row['sentences']
            sentence_count = row['sentence_count_check']
            
            # 确保sentences是列表
            if not isinstance(sentences, list):
                sentences = [sentences] if sentences else []
            
            # 过滤空句子
            sentences = [s for s in sentences if s and s.strip()]
            
            if not sentences:
                print(f"警告: 文档 {row['document_id']} 没有有效句子")
                continue
                
            full_text = ' '.join(sentences)
            
            # 1. 词汇熵
            word_entropy = self.compute_word_entropy(full_text)
            
            # 2. 句子长度熵
            length_entropy = self.compute_sentence_length_entropy(sentences)
            
            # 3. 语义熵
            semantic_entropy = self.compute_semantic_entropy(full_text)
            
            # 4. 信息密度
            info_density = self.compute_information_density(full_text)
            
            # 5. 文档长度（作为控制变量）
            doc_length = len(full_text.split())
            actual_sentence_count = len(sentences)
            
            entropy_signals.append({
                'qid': row['qid'],
                'document_id': row['document_id'],
                'filing_date': row['filing_date'],
                'word_entropy': word_entropy,
                'length_entropy': length_entropy,
                'semantic_entropy': semantic_entropy,
                'information_density': info_density,
                'doc_length': doc_length,
                'sentence_count': actual_sentence_count,
                'sentence_count_original': sentence_count  # 用于验证
            })
        
        entropy_df = pl.DataFrame(entropy_signals)
        
        # 计算复合信息量指标
        entropy_df = entropy_df.with_columns([
            # 综合信息量得分
            (pl.col('word_entropy') * 0.3 + 
             pl.col('semantic_entropy') * 0.3 + 
             pl.col('information_density') * 0.4).alias('composite_info_score'),
            
            # 标准化后的熵指标
            ((pl.col('word_entropy') - pl.col('word_entropy').mean()) / 
             pl.col('word_entropy').std()).alias('word_entropy_zscore'),
            
            ((pl.col('semantic_entropy') - pl.col('semantic_entropy').mean()) / 
             pl.col('semantic_entropy').std()).alias('semantic_entropy_zscore'),
            
            # 信息效率（信息量/长度）
            (pl.col('information_density') / pl.col('doc_length')).alias('info_efficiency')
        ])
        
        print(f"成功提取 {len(entropy_df)} 个文档的熵信号")
        return entropy_df
    
    def compute_cross_sectional_signals(self, entropy_df: pl.DataFrame) -> pl.DataFrame:
        """
        计算横截面信号 - 相对于同期其他filing的信息量
        """
        # 按日期计算横截面排名
        cross_sectional_df = entropy_df.with_columns([
            # 按日期计算分位数排名
            pl.col('composite_info_score').rank('dense').over('filing_date').alias('daily_info_rank'),
            pl.col('word_entropy').rank('dense').over('filing_date').alias('daily_word_entropy_rank'),
            pl.col('semantic_entropy').rank('dense').over('filing_date').alias('daily_semantic_entropy_rank'),
            
            # 计算日期内的相对位置
            (pl.col('composite_info_score').rank('dense').over('filing_date') / 
             pl.col('composite_info_score').count().over('filing_date')).alias('daily_info_percentile')
        ])
        
        # 识别异常高信息量的filing
        cross_sectional_df = cross_sectional_df.with_columns([
            # 高信息量标记
            (pl.col('daily_info_percentile') > 0.8).alias('high_info_flag'),
            
            # 信息量异常标记（超过2个标准差）
            (pl.col('word_entropy_zscore').abs() > 2).alias('entropy_anomaly_flag')
        ])
        
        return cross_sectional_df

# 使用示例
def analyze_filing_signals(df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
    """
    完整的8-K filing信号分析流程
    """
    analyzer = FilingEntropyAnalyzer(df)
    
    # 1. 提取基础熵信号
    entropy_signals = analyzer.extract_entropy_signals()
    
    # 2. 计算横截面信号
    cross_sectional_signals = analyzer.compute_cross_sectional_signals(entropy_signals)
    
    # 3. 生成最终的信号摘要
    signal_summary = cross_sectional_signals.select([
        'qid', 'document_id', 'filing_date',
        'composite_info_score', 'daily_info_percentile',
        'word_entropy', 'semantic_entropy', 'information_density',
        'high_info_flag', 'entropy_anomaly_flag'
    ]).sort(['filing_date', 'composite_info_score'], descending=[False, True])
    
    return {
        'entropy_signals': entropy_signals,
        'cross_sectional_signals': cross_sectional_signals,
        'signal_summary': signal_summary
    }

# 使用示例和数据验证
def validate_and_analyze(df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
    """
    验证数据结构并运行分析
    """
    print("=== 数据验证 ===")
    print(f"总行数: {len(df)}")
    print(f"唯一document_id数: {df['document_id'].n_unique()}")
    print(f"唯一qid数: {df['qid'].n_unique()}")
    
    # 检查一个document_id对应多个sentence_id的情况
    doc_sentence_counts = df.group_by('document_id').agg([
        pl.col('sentence_id').count().alias('sentence_count')
    ]).sort('sentence_count', descending=True)
    
    print(f"每个document的句子数分布:")
    print(doc_sentence_counts.head(10))
    
    max_sentences = doc_sentence_counts['sentence_count'].max()
    if max_sentences > 1:
        print(f"✓ 数据包含一个document_id对应多个sentence_id的情况，最多{max_sentences}个句子")
    else:
        print("! 每个document_id只对应一个sentence_id")
    
    # 运行分析
    print("\n=== 开始分析 ===")
    return analyze_filing_signals(df)

# 示例用法 - 包含一个document对应多个sentence的情况
def create_sample_data():
    """创建示例数据，包含一个document对应多个sentence的情况"""
    return pl.DataFrame({
        'qid': ['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q3'],
        'document_id': ['D1', 'D1', 'D1', 'D2', 'D2', 'D3'],  # D1有3个句子，D2有2个句子
        'sentence_id': ['S1', 'S2', 'S3', 'S4', 'S5', 'S6'],
        'sentence_text': [
            'Company announces major acquisition of technology firm for $2.5 billion cash deal.',
            'The acquisition will strengthen our artificial intelligence capabilities significantly.',
            'We expect synergies of $500 million annually by year three post-closing.',
            'Quarterly earnings exceeded analyst expectations with revenue growth of 15%.',
            'Strong performance driven by increased demand in our core markets.',
            'New regulatory compliance measures implemented across all business units.'
        ],
        'filing_date': ['2024-01-15', '2024-01-15', '2024-01-15', '2024-01-16', '2024-01-16', '2024-01-17']
    })

# 运行示例
"""
# 创建示例数据
sample_df = create_sample_data()

# 验证并分析
results = validate_and_analyze(sample_df)

# 查看结果
print("\n=== 信号摘要 ===")
print(results['signal_summary'])

print("\n=== 熵信号详情 ===")
print(results['entropy_signals'].select([
    'document_id', 'sentence_count', 'word_entropy', 
    'semantic_entropy', 'information_density', 'composite_info_score'
]))
"""

====================================
import polars as pl
import numpy as np
from collections import Counter
from typing import Dict, List, Tuple
from scipy.stats import entropy as scipy_entropy
import re
from datetime import datetime, timedelta
import math

class FilingEntropyAnalyzer:
    """
    基于信息论的8-K Filing信号提取器
    核心思想：通过多维度的熵计算来量化filing的信息含量
    """
    
    def __init__(self, df: pl.DataFrame):
        """
        初始化分析器
        
        参数:
        df: 包含columns [qid, document_id, sentence_id, sentence_text, filing_date]
        """
        self.df = df
        self.stopwords = self._get_financial_stopwords()
        
    def _get_financial_stopwords(self) -> set:
        """获取金融领域的停用词"""
        basic_stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'as', 'is', 'was', 'are', 'were', 'be', 'been',
            'have', 'has', 'had', 'will', 'would', 'could', 'should', 'may', 'might',
            'this', 'that', 'these', 'those', 'such', 'said', 'says'
        }
        
        # 添加金融相关的常见词汇（这些词信息量较低）
        financial_common = {
            'company', 'corporation', 'inc', 'llc', 'ltd', 'securities', 'sec',
            'pursuant', 'accordance', 'herewith', 'thereof', 'hereby', 'whereas'
        }
        
        return basic_stopwords.union(financial_common)
    
    def compute_word_entropy(self, text: str) -> float:
        """
        计算单个文本的词汇熵
        H(X) = -Σ p(x) * log2(p(x))
        """
        # 文本预处理
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        words = [w for w in words if w not in self.stopwords and len(w) > 2]
        
        if not words:
            return 0.0
            
        # 计算词频
        word_counts = Counter(words)
        total_words = len(words)
        
        # 计算概率分布
        probabilities = [count / total_words for count in word_counts.values()]
        
        # 计算熵
        return scipy_entropy(probabilities, base=2)
    
    def compute_sentence_length_entropy(self, sentences: List[str]) -> float:
        """
        计算句子长度分布的熵
        长度分布的不均匀性可能反映信息的结构化程度
        """
        if not sentences:
            return 0.0
            
        lengths = [len(sentence.split()) for sentence in sentences]
        
        # 将长度分桶
        max_len = max(lengths)
        bins = np.linspace(0, max_len, min(10, max_len + 1))
        hist, _ = np.histogram(lengths, bins=bins)
        
        # 过滤掉空桶
        hist = hist[hist > 0]
        probabilities = hist / hist.sum()
        
        return scipy_entropy(probabilities, base=2)
    
    def compute_semantic_entropy(self, text: str) -> float:
        """
        计算语义熵 - 基于关键词类别的分布
        """
        # 定义不同类别的关键词
        categories = {
            'financial': ['revenue', 'profit', 'loss', 'earnings', 'cash', 'debt', 'asset', 'liability', 'dividend'],
            'legal': ['agreement', 'contract', 'lawsuit', 'litigation', 'settlement', 'compliance', 'regulation'],
            'operational': ['acquisition', 'merger', 'restructuring', 'expansion', 'closure', 'partnership'],
            'personnel': ['appointment', 'resignation', 'executive', 'director', 'officer', 'employment'],
            'market': ['stock', 'share', 'market', 'trading', 'price', 'valuation', 'investor'],
            'risk': ['risk', 'uncertainty', 'contingency', 'material', 'adverse', 'impact', 'exposure']
        }
        
        text_lower = text.lower()
        category_counts = {}
        
        for category, keywords in categories.items():
            count = sum(text_lower.count(keyword) for keyword in keywords)
            if count > 0:
                category_counts[category] = count
        
        if not category_counts:
            return 0.0
            
        total_matches = sum(category_counts.values())
        probabilities = [count / total_matches for count in category_counts.values()]
        
        return scipy_entropy(probabilities, base=2)
    
    def compute_temporal_surprise(self, current_date: str, lookback_days: int = 30) -> pl.DataFrame:
        """
        计算时间维度的惊喜度（相对熵）
        通过比较当前filing与历史filing的词汇分布差异
        """
        current_date_dt = datetime.strptime(current_date, '%Y-%m-%d')
        lookback_date = current_date_dt - timedelta(days=lookback_days)
        
        # 获取历史数据
        historical_df = self.df.filter(
            (pl.col('filing_date') >= lookback_date.strftime('%Y-%m-%d')) &
            (pl.col('filing_date') < current_date)
        )
        
        current_df = self.df.filter(pl.col('filing_date') == current_date)
        
        if historical_df.is_empty() or current_df.is_empty():
            return current_df.with_columns([
                pl.lit(0.0).alias('temporal_surprise')
            ])
        
        # 构建历史词汇分布
        historical_text = ' '.join(historical_df['sentence_text'].to_list())
        historical_words = re.findall(r'\b[a-zA-Z]+\b', historical_text.lower())
        historical_words = [w for w in historical_words if w not in self.stopwords and len(w) > 2]
        historical_dist = Counter(historical_words)
        
        # 计算当前每个document的KL散度
        surprise_scores = []
        
        for doc_id in current_df['document_id'].unique():
            doc_text = ' '.join(
                current_df.filter(pl.col('document_id') == doc_id)['sentence_text'].to_list()
            )
            
            current_words = re.findall(r'\b[a-zA-Z]+\b', doc_text.lower())
            current_words = [w for w in current_words if w not in self.stopwords and len(w) > 2]
            current_dist = Counter(current_words)
            
            # 计算KL散度
            kl_div = self._compute_kl_divergence(current_dist, historical_dist)
            surprise_scores.append({'document_id': doc_id, 'temporal_surprise': kl_div})
        
        surprise_df = pl.DataFrame(surprise_scores)
        return current_df.join(surprise_df, on='document_id', how='left')
    
    def _compute_kl_divergence(self, p_dist: Counter, q_dist: Counter) -> float:
        """
        计算两个分布之间的KL散度
        KL(P||Q) = Σ P(x) * log(P(x)/Q(x))
        """
        if not p_dist or not q_dist:
            return 0.0
            
        # 获取所有词汇
        all_words = set(p_dist.keys()) | set(q_dist.keys())
        
        # 平滑处理
        alpha = 0.001
        p_total = sum(p_dist.values())
        q_total = sum(q_dist.values())
        
        kl_div = 0.0
        for word in all_words:
            p_prob = (p_dist.get(word, 0) + alpha) / (p_total + alpha * len(all_words))
            q_prob = (q_dist.get(word, 0) + alpha) / (q_total + alpha * len(all_words))
            
            kl_div += p_prob * math.log(p_prob / q_prob)
        
        return kl_div
    
    def compute_information_density(self, text: str) -> float:
        """
        计算信息密度 - 每个词的平均信息量
        """
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        words = [w for w in words if w not in self.stopwords and len(w) > 2]
        
        if not words:
            return 0.0
            
        # 使用逆文档频率作为信息量的代理
        word_counts = Counter(words)
        total_words = len(words)
        
        # 计算每个词的信息量 (负对数概率)
        information_content = 0.0
        for word, count in word_counts.items():
            prob = count / total_words
            information_content += count * (-math.log2(prob))
        
        return information_content / total_words
    
    def extract_entropy_signals(self) -> pl.DataFrame:
        """
        提取所有基于熵的信号
        """
        print("开始提取熵信号...")
        
        # 按document聚合句子
        doc_level_df = self.df.group_by(['qid', 'document_id', 'filing_date']).agg([
            pl.col('sentence_text').alias('sentences')
        ])
        
        # 计算各种熵指标
        entropy_signals = []
        
        for row in doc_level_df.iter_rows(named=True):
            sentences = row['sentences']
            full_text = ' '.join(sentences)
            
            # 1. 词汇熵
            word_entropy = self.compute_word_entropy(full_text)
            
            # 2. 句子长度熵
            length_entropy = self.compute_sentence_length_entropy(sentences)
            
            # 3. 语义熵
            semantic_entropy = self.compute_semantic_entropy(full_text)
            
            # 4. 信息密度
            info_density = self.compute_information_density(full_text)
            
            # 5. 文档长度（作为控制变量）
            doc_length = len(full_text.split())
            sentence_count = len(sentences)
            
            entropy_signals.append({
                'qid': row['qid'],
                'document_id': row['document_id'],
                'filing_date': row['filing_date'],
                'word_entropy': word_entropy,
                'length_entropy': length_entropy,
                'semantic_entropy': semantic_entropy,
                'information_density': info_density,
                'doc_length': doc_length,
                'sentence_count': sentence_count
            })
        
        entropy_df = pl.DataFrame(entropy_signals)
        
        # 计算复合信息量指标
        entropy_df = entropy_df.with_columns([
            # 综合信息量得分
            (pl.col('word_entropy') * 0.3 + 
             pl.col('semantic_entropy') * 0.3 + 
             pl.col('information_density') * 0.4).alias('composite_info_score'),
            
            # 标准化后的熵指标
            ((pl.col('word_entropy') - pl.col('word_entropy').mean()) / 
             pl.col('word_entropy').std()).alias('word_entropy_zscore'),
            
            ((pl.col('semantic_entropy') - pl.col('semantic_entropy').mean()) / 
             pl.col('semantic_entropy').std()).alias('semantic_entropy_zscore'),
            
            # 信息效率（信息量/长度）
            (pl.col('information_density') / pl.col('doc_length')).alias('info_efficiency')
        ])
        
        print(f"成功提取 {len(entropy_df)} 个文档的熵信号")
        return entropy_df
    
    def compute_cross_sectional_signals(self, entropy_df: pl.DataFrame) -> pl.DataFrame:
        """
        计算横截面信号 - 相对于同期其他filing的信息量
        """
        # 按日期计算横截面排名
        cross_sectional_df = entropy_df.with_columns([
            # 按日期计算分位数排名
            pl.col('composite_info_score').rank('dense').over('filing_date').alias('daily_info_rank'),
            pl.col('word_entropy').rank('dense').over('filing_date').alias('daily_word_entropy_rank'),
            pl.col('semantic_entropy').rank('dense').over('filing_date').alias('daily_semantic_entropy_rank'),
            
            # 计算日期内的相对位置
            (pl.col('composite_info_score').rank('dense').over('filing_date') / 
             pl.col('composite_info_score').count().over('filing_date')).alias('daily_info_percentile')
        ])
        
        # 识别异常高信息量的filing
        cross_sectional_df = cross_sectional_df.with_columns([
            # 高信息量标记
            (pl.col('daily_info_percentile') > 0.8).alias('high_info_flag'),
            
            # 信息量异常标记（超过2个标准差）
            (pl.col('word_entropy_zscore').abs() > 2).alias('entropy_anomaly_flag')
        ])
        
        return cross_sectional_df

# 使用示例
def analyze_filing_signals(df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
    """
    完整的8-K filing信号分析流程
    """
    analyzer = FilingEntropyAnalyzer(df)
    
    # 1. 提取基础熵信号
    entropy_signals = analyzer.extract_entropy_signals()
    
    # 2. 计算横截面信号
    cross_sectional_signals = analyzer.compute_cross_sectional_signals(entropy_signals)
    
    # 3. 生成最终的信号摘要
    signal_summary = cross_sectional_signals.select([
        'qid', 'document_id', 'filing_date',
        'composite_info_score', 'daily_info_percentile',
        'word_entropy', 'semantic_entropy', 'information_density',
        'high_info_flag', 'entropy_anomaly_flag'
    ]).sort(['filing_date', 'composite_info_score'], descending=[False, True])
    
    return {
        'entropy_signals': entropy_signals,
        'cross_sectional_signals': cross_sectional_signals,
        'signal_summary': signal_summary
    }

# 示例用法
"""
# 假设你有一个DataFrame
df = pl.DataFrame({
    'qid': ['Q1', 'Q2', 'Q3'],
    'document_id': ['D1', 'D2', 'D3'],
    'sentence_id': ['S1', 'S2', 'S3'],
    'sentence_text': ['Company announces major acquisition...', 
                      'Quarterly earnings exceeded expectations...',
                      'New regulatory compliance measures...'],
    'filing_date': ['2024-01-15', '2024-01-16', '2024-01-17']
})

# 运行分析
results = analyze_filing_signals(df)
print(results['signal_summary'])
"""
