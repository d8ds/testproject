# testproject

TD: This is a test generated by AI


import polars as pl
import numpy as np
from collections import Counter
from typing import Dict, List, Tuple
from scipy.stats import entropy as scipy_entropy
import re
from datetime import datetime, timedelta
import math

class FilingEntropyAnalyzer:
    """
    基于信息论的8-K Filing信号提取器
    核心思想：通过多维度的熵计算来量化filing的信息含量
    """
    
    def __init__(self, df: pl.DataFrame):
        """
        初始化分析器
        
        参数:
        df: 包含columns [qid, document_id, sentence_id, sentence_text, filing_date]
        """
        self.df = df
        self.stopwords = self._get_financial_stopwords()
        
    def _get_financial_stopwords(self) -> set:
        """获取金融领域的停用词"""
        basic_stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 
            'of', 'with', 'by', 'as', 'is', 'was', 'are', 'were', 'be', 'been',
            'have', 'has', 'had', 'will', 'would', 'could', 'should', 'may', 'might',
            'this', 'that', 'these', 'those', 'such', 'said', 'says'
        }
        
        # 添加金融相关的常见词汇（这些词信息量较低）
        financial_common = {
            'company', 'corporation', 'inc', 'llc', 'ltd', 'securities', 'sec',
            'pursuant', 'accordance', 'herewith', 'thereof', 'hereby', 'whereas'
        }
        
        return basic_stopwords.union(financial_common)
    
    def compute_word_entropy(self, text: str) -> float:
        """
        计算单个文本的词汇熵
        H(X) = -Σ p(x) * log2(p(x))
        """
        # 文本预处理
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        words = [w for w in words if w not in self.stopwords and len(w) > 2]
        
        if not words:
            return 0.0
            
        # 计算词频
        word_counts = Counter(words)
        total_words = len(words)
        
        # 计算概率分布
        probabilities = [count / total_words for count in word_counts.values()]
        
        # 计算熵
        return scipy_entropy(probabilities, base=2)
    
    def compute_sentence_length_entropy(self, sentences: List[str]) -> float:
        """
        计算句子长度分布的熵
        长度分布的不均匀性可能反映信息的结构化程度
        """
        if not sentences:
            return 0.0
            
        lengths = [len(sentence.split()) for sentence in sentences]
        
        # 将长度分桶
        max_len = max(lengths)
        bins = np.linspace(0, max_len, min(10, max_len + 1))
        hist, _ = np.histogram(lengths, bins=bins)
        
        # 过滤掉空桶
        hist = hist[hist > 0]
        probabilities = hist / hist.sum()
        
        return scipy_entropy(probabilities, base=2)
    
    def compute_semantic_entropy(self, text: str) -> float:
        """
        计算语义熵 - 基于关键词类别的分布
        """
        # 定义不同类别的关键词
        categories = {
            'financial': ['revenue', 'profit', 'loss', 'earnings', 'cash', 'debt', 'asset', 'liability', 'dividend'],
            'legal': ['agreement', 'contract', 'lawsuit', 'litigation', 'settlement', 'compliance', 'regulation'],
            'operational': ['acquisition', 'merger', 'restructuring', 'expansion', 'closure', 'partnership'],
            'personnel': ['appointment', 'resignation', 'executive', 'director', 'officer', 'employment'],
            'market': ['stock', 'share', 'market', 'trading', 'price', 'valuation', 'investor'],
            'risk': ['risk', 'uncertainty', 'contingency', 'material', 'adverse', 'impact', 'exposure']
        }
        
        text_lower = text.lower()
        category_counts = {}
        
        for category, keywords in categories.items():
            count = sum(text_lower.count(keyword) for keyword in keywords)
            if count > 0:
                category_counts[category] = count
        
        if not category_counts:
            return 0.0
            
        total_matches = sum(category_counts.values())
        probabilities = [count / total_matches for count in category_counts.values()]
        
        return scipy_entropy(probabilities, base=2)
    
    def compute_temporal_surprise(self, current_date: str, lookback_days: int = 30) -> pl.DataFrame:
        """
        计算时间维度的惊喜度（相对熵）
        通过比较当前filing与历史filing的词汇分布差异
        """
        current_date_dt = datetime.strptime(current_date, '%Y-%m-%d')
        lookback_date = current_date_dt - timedelta(days=lookback_days)
        
        # 获取历史数据
        historical_df = self.df.filter(
            (pl.col('filing_date') >= lookback_date.strftime('%Y-%m-%d')) &
            (pl.col('filing_date') < current_date)
        )
        
        current_df = self.df.filter(pl.col('filing_date') == current_date)
        
        if historical_df.is_empty() or current_df.is_empty():
            return current_df.with_columns([
                pl.lit(0.0).alias('temporal_surprise')
            ])
        
        # 构建历史词汇分布
        historical_text = ' '.join(historical_df['sentence_text'].to_list())
        historical_words = re.findall(r'\b[a-zA-Z]+\b', historical_text.lower())
        historical_words = [w for w in historical_words if w not in self.stopwords and len(w) > 2]
        historical_dist = Counter(historical_words)
        
        # 计算当前每个document的KL散度
        surprise_scores = []
        
        for doc_id in current_df['document_id'].unique():
            doc_text = ' '.join(
                current_df.filter(pl.col('document_id') == doc_id)['sentence_text'].to_list()
            )
            
            current_words = re.findall(r'\b[a-zA-Z]+\b', doc_text.lower())
            current_words = [w for w in current_words if w not in self.stopwords and len(w) > 2]
            current_dist = Counter(current_words)
            
            # 计算KL散度
            kl_div = self._compute_kl_divergence(current_dist, historical_dist)
            surprise_scores.append({'document_id': doc_id, 'temporal_surprise': kl_div})
        
        surprise_df = pl.DataFrame(surprise_scores)
        return current_df.join(surprise_df, on='document_id', how='left')
    
    def _compute_kl_divergence(self, p_dist: Counter, q_dist: Counter) -> float:
        """
        计算两个分布之间的KL散度
        KL(P||Q) = Σ P(x) * log(P(x)/Q(x))
        """
        if not p_dist or not q_dist:
            return 0.0
            
        # 获取所有词汇
        all_words = set(p_dist.keys()) | set(q_dist.keys())
        
        # 平滑处理
        alpha = 0.001
        p_total = sum(p_dist.values())
        q_total = sum(q_dist.values())
        
        kl_div = 0.0
        for word in all_words:
            p_prob = (p_dist.get(word, 0) + alpha) / (p_total + alpha * len(all_words))
            q_prob = (q_dist.get(word, 0) + alpha) / (q_total + alpha * len(all_words))
            
            kl_div += p_prob * math.log(p_prob / q_prob)
        
        return kl_div
    
    def compute_information_density(self, text: str) -> float:
        """
        计算信息密度 - 每个词的平均信息量
        """
        words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
        words = [w for w in words if w not in self.stopwords and len(w) > 2]
        
        if not words:
            return 0.0
            
        # 使用逆文档频率作为信息量的代理
        word_counts = Counter(words)
        total_words = len(words)
        
        # 计算每个词的信息量 (负对数概率)
        information_content = 0.0
        for word, count in word_counts.items():
            prob = count / total_words
            information_content += count * (-math.log2(prob))
        
        return information_content / total_words
    
    def extract_entropy_signals(self) -> pl.DataFrame:
        """
        提取所有基于熵的信号
        """
        print("开始提取熵信号...")
        
        # 按document聚合句子
        doc_level_df = self.df.group_by(['qid', 'document_id', 'filing_date']).agg([
            pl.col('sentence_text').alias('sentences')
        ])
        
        # 计算各种熵指标
        entropy_signals = []
        
        for row in doc_level_df.iter_rows(named=True):
            sentences = row['sentences']
            full_text = ' '.join(sentences)
            
            # 1. 词汇熵
            word_entropy = self.compute_word_entropy(full_text)
            
            # 2. 句子长度熵
            length_entropy = self.compute_sentence_length_entropy(sentences)
            
            # 3. 语义熵
            semantic_entropy = self.compute_semantic_entropy(full_text)
            
            # 4. 信息密度
            info_density = self.compute_information_density(full_text)
            
            # 5. 文档长度（作为控制变量）
            doc_length = len(full_text.split())
            sentence_count = len(sentences)
            
            entropy_signals.append({
                'qid': row['qid'],
                'document_id': row['document_id'],
                'filing_date': row['filing_date'],
                'word_entropy': word_entropy,
                'length_entropy': length_entropy,
                'semantic_entropy': semantic_entropy,
                'information_density': info_density,
                'doc_length': doc_length,
                'sentence_count': sentence_count
            })
        
        entropy_df = pl.DataFrame(entropy_signals)
        
        # 计算复合信息量指标
        entropy_df = entropy_df.with_columns([
            # 综合信息量得分
            (pl.col('word_entropy') * 0.3 + 
             pl.col('semantic_entropy') * 0.3 + 
             pl.col('information_density') * 0.4).alias('composite_info_score'),
            
            # 标准化后的熵指标
            ((pl.col('word_entropy') - pl.col('word_entropy').mean()) / 
             pl.col('word_entropy').std()).alias('word_entropy_zscore'),
            
            ((pl.col('semantic_entropy') - pl.col('semantic_entropy').mean()) / 
             pl.col('semantic_entropy').std()).alias('semantic_entropy_zscore'),
            
            # 信息效率（信息量/长度）
            (pl.col('information_density') / pl.col('doc_length')).alias('info_efficiency')
        ])
        
        print(f"成功提取 {len(entropy_df)} 个文档的熵信号")
        return entropy_df
    
    def compute_cross_sectional_signals(self, entropy_df: pl.DataFrame) -> pl.DataFrame:
        """
        计算横截面信号 - 相对于同期其他filing的信息量
        """
        # 按日期计算横截面排名
        cross_sectional_df = entropy_df.with_columns([
            # 按日期计算分位数排名
            pl.col('composite_info_score').rank('dense').over('filing_date').alias('daily_info_rank'),
            pl.col('word_entropy').rank('dense').over('filing_date').alias('daily_word_entropy_rank'),
            pl.col('semantic_entropy').rank('dense').over('filing_date').alias('daily_semantic_entropy_rank'),
            
            # 计算日期内的相对位置
            (pl.col('composite_info_score').rank('dense').over('filing_date') / 
             pl.col('composite_info_score').count().over('filing_date')).alias('daily_info_percentile')
        ])
        
        # 识别异常高信息量的filing
        cross_sectional_df = cross_sectional_df.with_columns([
            # 高信息量标记
            (pl.col('daily_info_percentile') > 0.8).alias('high_info_flag'),
            
            # 信息量异常标记（超过2个标准差）
            (pl.col('word_entropy_zscore').abs() > 2).alias('entropy_anomaly_flag')
        ])
        
        return cross_sectional_df

# 使用示例
def analyze_filing_signals(df: pl.DataFrame) -> Dict[str, pl.DataFrame]:
    """
    完整的8-K filing信号分析流程
    """
    analyzer = FilingEntropyAnalyzer(df)
    
    # 1. 提取基础熵信号
    entropy_signals = analyzer.extract_entropy_signals()
    
    # 2. 计算横截面信号
    cross_sectional_signals = analyzer.compute_cross_sectional_signals(entropy_signals)
    
    # 3. 生成最终的信号摘要
    signal_summary = cross_sectional_signals.select([
        'qid', 'document_id', 'filing_date',
        'composite_info_score', 'daily_info_percentile',
        'word_entropy', 'semantic_entropy', 'information_density',
        'high_info_flag', 'entropy_anomaly_flag'
    ]).sort(['filing_date', 'composite_info_score'], descending=[False, True])
    
    return {
        'entropy_signals': entropy_signals,
        'cross_sectional_signals': cross_sectional_signals,
        'signal_summary': signal_summary
    }

# 示例用法
"""
# 假设你有一个DataFrame
df = pl.DataFrame({
    'qid': ['Q1', 'Q2', 'Q3'],
    'document_id': ['D1', 'D2', 'D3'],
    'sentence_id': ['S1', 'S2', 'S3'],
    'sentence_text': ['Company announces major acquisition...', 
                      'Quarterly earnings exceeded expectations...',
                      'New regulatory compliance measures...'],
    'filing_date': ['2024-01-15', '2024-01-16', '2024-01-17']
})

# 运行分析
results = analyze_filing_signals(df)
print(results['signal_summary'])
"""
