Excellent idea! This adds credibility and shows intellectual honesty. Here are several options:

---

## **Option 1: "What Didn't Work" (Most Direct)**

**Title:** "What Didn't Work: Lessons from Failed Signals"

**Subtitle:** "Not every hypothesis panned out—here's what we learned"

**Content Structure:**

| Signal Tested | Hypothesis | Why We Thought It Would Work | Result | Why It Failed |
|---------------|------------|------------------------------|--------|---------------|
| **Readability Scores** (Flesch-Kincaid, etc.) | Complex writing = obfuscation | Harder-to-read filings hide bad news | IC ≈ 0.00 | 8-Ks are already simple; little cross-sectional variation |
| **Sentiment Analysis** (Basic tone) | Negative tone = bad news | Pessimistic language predicts returns | IC ≈ 0.01 | Management tone in 8-Ks is formulaic/neutral; limited signal |
| **Filing Size (File KB)** | Large files = complexity | File size includes exhibits, complexity | IC ≈ 0.00 | Driven by PDF formatting, not content; noisy proxy |
| **Word Diversity** (Unique words/Total words) | High diversity = substantive | Rich vocabulary indicates depth | IC ≈ -0.01 | Legal boilerplate has high diversity; inverse of intent |
| **Average Sentence Length** | Long sentences = complexity | Verbose syntax signals uncertainty | IC ≈ 0.00 | No consistent pattern; too granular |

**Key Learnings:**

1. **Generic NLP metrics underperform domain-specific features**
   - Readability and sentiment work better for 10-Ks (longer, more narrative)
   - 8-Ks are too short and formulaic for these to differentiate

2. **File-level metadata is too noisy**
   - PDF compression, formatting choices dominate actual content signals
   - Better to work with extracted text

3. **Sophisticated ≠ Predictive**
   - Word diversity seemed clever but had no economic intuition in retrospect
   - Simpler, behavior-focused metrics (delay, length) outperformed

**The Takeaway:**
> "We tested many sophisticated NLP features. What worked best? Simple, interpretable metrics applied with rolling aggregation. **Methodology matters more than feature complexity.**"

---

## **Option 2: "Signal Graveyard" (More Memorable)**

**Title:** "The Signal Graveyard: Promising Ideas That Didn't Pan Out"

**Visual:** Tombstone graphics or a graveyard theme (can be tasteful/professional)

**Structure:** Brief obituaries for each failed signal

**Example Entry:**

```
⚰️ SENTIMENT ANALYSIS
   "Negative tone predicts negative returns"
   
   Tested: 2023-2024
   Result: IC = 0.01, Sharpe = 0.2
   
   Cause of Death: 8-K language is too formulaic and 
   legally vetted. Little variation in tone within the 
   event-driven context. Management saves sentiment 
   expression for earnings calls, not SEC filings.
   
   R.I.P.
```

**Other Tombstones:**
- Readability Scores
- File Size Metrics  
- Vocabulary Diversity
- Average Sentence Length
- Table/Exhibit Count (unless this worked?)

**The Message:**
> "Good research requires testing what **doesn't** work. These failures guided us toward signals with clearer behavioral interpretation and stronger theoretical foundations."

---

## **Option 3: "Negative Results" (Academic Tone)**

**Title:** "Negative Results: Signals That Showed No Predictive Power"

**Subtitle:** "Transparency in what we tested and why some signals failed"

**Table Format:**

| Category | Signal | IC | Sharpe | Why It Failed |
|----------|--------|-----|--------|---------------|
| **Text Complexity** | Flesch Reading Ease | 0.00 | 0.1 | Insufficient variation in 8-Ks |
| | Fog Index | 0.00 | 0.1 | Same issue; 8-Ks already simple |
| | Average Sentence Length | 0.01 | 0.2 | Too granular; no clear pattern |
| **Sentiment** | Loughran-McDonald Negative | 0.01 | 0.2 | 8-Ks lack emotional language |
| | Net Sentiment Score | 0.00 | 0.1 | Management tone too neutral |
| **Structural** | File Size (KB) | 0.00 | 0.0 | PDF formatting noise |
| | Exhibit Count | 0.01 | 0.2 | Weakly predictive, redundant with sections |
| **Lexical** | Vocabulary Diversity (TTR) | -0.01 | -0.1 | Counterintuitive; boilerplate has high diversity |
| | Unique Word Count | 0.00 | 0.1 | Correlated with length but no marginal value |

**Discussion Points:**

**Why Text Complexity Failed:**
- 8-Ks have narrow readability range (grade 12-14)
- Unlike 10-Ks which vary from grade 10-18
- Insufficient cross-sectional variation to extract signal

**Why Sentiment Failed:**
- Regulatory language is deliberately neutral
- Management avoids charged language in 8-Ks
- Sentiment better suited for MD&A sections in 10-K/Q

**Why File Metadata Failed:**
- Technical artifacts (PDF version, compression) add noise
- Text-based features more robust

**The Learning:**
> "These negative results reinforced our focus on **behavioral** rather than **linguistic** signals. Management can control word choice; they reveal more through structure, timing, and disclosure patterns."

---

## **Option 4: "Lessons from the Cutting Room Floor" (Narrative)**

**Title:** "Lessons from the Cutting Room Floor"

**Subtitle:** "Features we tested but ultimately excluded—and why"

**Story Format:**

**"The Sophistication Trap"**

"Early in our research, we pursued sophisticated NLP features:
- Sentiment scores using Loughran-McDonald dictionary
- Readability metrics (Flesch-Kincaid, Gunning Fog)
- Lexical diversity measures

**The Problem:** These metrics work well for longer, narrative documents (analyst reports, 10-Ks). For short, event-driven 8-Ks with legal review, they showed minimal variation and no predictive power.

**IC Range:** -0.01 to +0.01 (statistically zero)

**The Lesson:** Domain matters. Tools that work for earnings calls don't necessarily work for regulatory filings."

**"The Metadata Mirage"**

"We hypothesized that file-level characteristics might signal something:
- File size in KB (larger = more complex)
- Number of exhibits attached
- Presence of XBRL tags

**The Problem:** File size is dominated by PDF formatting choices, not content. Exhibit counts weakly correlated with section counts but added no marginal information.

**The Lesson:** Go straight to content. Metadata proxies introduce noise without adding signal."

**"The Vocabulary Puzzle"**

"We tested whether lexical diversity (unique words / total words) indicated disclosure quality.

**The Irony:** Boilerplate legal language has *high* vocabulary diversity. Simple, direct disclosures actually have *lower* diversity.

**Result:** Signal had *negative* correlation with returns (opposite of hypothesis)

**The Lesson:** Linguistic features need strong economic intuition. Complexity metrics can be counterintuitive."

---

## **Option 5: "The Feature Selection Journey" (Methodological)**

**Title:** "Feature Selection: From 20+ Candidates to 6 Core Signals"

**Visual:** Funnel diagram showing the filtering process

**Stage 1: Initial Brainstorm (20+ features)**
```
Text Features: Length, readability, sentiment, diversity, sentence metrics
Structural: Sections, exhibits, tables, item types
Metadata: File size, submission time, amendments
Numerical: Number density, precision, percentage counts
Behavioral: Filing delay, frequency, clustering
```

**Stage 2: Univariate Screening (Filter: |IC| > 0.02)**
```
Passed (10 features):
✓ Length, Filtered Length, Number Density
✓ Section Count, Material Length
✓ Filing Delay, Filing Frequency
✓ Item Diversity
✗ Failed: Readability, Sentiment, File Size, etc.
```

**Stage 3: Rolling Transformation (Filter: IC improvement > 50%)**
```
Strong Improvement (6 features):
✓ All length-based metrics
✓ All structural metrics  
✓ Filing delay

Minimal Improvement (4 features):
~ Item diversity (marginal)
~ Filing frequency (redundant with count metrics)
```

**Stage 4: Correlation Analysis (Filter: ρ < 0.80)**
```
Final Selection (6 features):
✓ Relatively orthogonal (ρ = 0.10-0.35)
✓ Diverse dimensions (content, structure, behavior)
✓ Strong individual performance
✓ Complementary information
```

**The Takeaway:**
> "Rigorous feature selection matters. We tested 3x more signals than we kept. The final six survived because they're interpretable, orthogonal, and consistently predictive."

---

## **My Recommendation: Option 3 + Elements of Option 1**

**Title:** "What Didn't Work: Negative Results and Lessons Learned"

**Layout:**

**Top Section (30%):** Brief table showing failed signals with ICs

**Middle Section (40%):** Three key lessons in boxes:

```
┌─────────────────────────────────────────────────┐
│ LESSON 1: Linguistic Complexity ≠ Alpha         │
│                                                  │
│ Readability and sentiment metrics work for      │
│ narrative documents (10-Ks, earnings calls)     │
│ but not for short, legally-reviewed 8-Ks.       │
│                                                  │
│ Why: Insufficient cross-sectional variation     │
└─────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────┐
│ LESSON 2: Behavioral > Linguistic               │
│                                                  │
│ Management can carefully craft word choice.     │
│ But behavioral signals (timing, patterns)       │
│ are harder to manipulate.                       │
│                                                  │
│ Result: Delay (IC=0.08) >> Sentiment (IC=0.01) │
└─────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────┐
│ LESSON 3: Methodology > Feature Sophistication  │
│                                                  │
│ Simple features (length) with rolling           │
│ aggregation outperformed sophisticated          │
│ features (vocabulary diversity) with            │
│ point-based measurement.                        │
│                                                  │
│ Takeaway: How you measure matters more than     │
│ what you measure                                │
└─────────────────────────────────────────────────┘
```

**Bottom Section (30%):** Brief commentary

**The Message:**
> "Strong research requires testing what **doesn't** work. These negative results guided our final framework toward behaviorally-grounded, interpretable signals with clear economic intuition. Publication bias favors only showing winners—we believe transparency about failures strengthens our conclusions."

---

## **Additional Suggestions**

**Consider Adding:**

1. **One Surprising Anti-Signal**
   - Something that worked in the *opposite* direction of hypothesis
   - Example: "High vocabulary diversity *negatively* predicted returns (IC = -0.01)"
   - Shows you're not p-hacking; you found counterintuitive results

2. **Resource Allocation Note**
   - "We spent 30% of research time on signals that ultimately failed"
   - Shows thoroughness and scientific rigor

3. **Forward-Looking Teaser**
   - "Some signals (like sentiment) might work better in different contexts"
   - "Future work: Test these features on 10-K MD&A sections"

---

## **Presentation Tips for This Slide**

**Timing:** 60-90 seconds max
- Don't dwell, but don't rush
- Confidence is key: "We tested this rigorously; here's what didn't work"

**Tone:** 
- Matter-of-fact, not defensive
- "This is science; negative results are part of the process"
- Slightly lighter than other slides (can use gentle humor: "RIP sentiment analysis")

**Transitions:**
- **Into:** "Before we wrap up, let's be transparent about what we tested but ultimately rejected..."
- **Out of:** "These negative results clarified our thinking and led us to the six robust signals you've seen today. Now, let's discuss next steps..."

**Why This Works for Senior Audience:**
- Shows intellectual honesty (not cherry-picking)
- Demonstrates thorough research process
- Preempts "did you try X?" questions
- Builds trust in your conclusions

---

Would you like me to create a specific layout for one of these options, or combine elements from multiple approaches?
